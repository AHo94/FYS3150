\documentclass[12pt]{article}
\author{Alex Ho}
\title{FYS4150 - Computational Physics \\ Project 2}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage[utf8]{inputenc}
\usepackage[english, norsk]{babel}
\usepackage{xcolor}
\usepackage{hyperref}

\lstset{
language=Python,
basicstyle=\ttfamily,
otherkeywords={self},             
keywordstyle=\ttfamily\color{blue!90!black},
keywords=[2]{True,False},
keywordstyle={[2]\ttfamily\color{blue!90!black}},
emph={MyClass,__init__},          
emphstyle=\ttfamily\color{red!80!black},    
stringstyle=\color{blue!90!black},
showstringspaces=false,
commentstyle=\color{blue!90!black},
breaklines=true,
tabsize=3,
moredelim=**[is][\color{blue}]{@}{@}
}

\begin{document}
\maketitle
\section{Introduction}
In this project, we will have a look at the so called \textit{Eigenvalue problem}. We will use the well known Schrödinger equation from quantum mechanics and apply it to a system consisting of a single electron in a harmonic oscillator potential. To achieve this, we will implement Jacobi's method to solve the Schrödinger equation and then find the eigenvalues, which will physically be the energies of the electrons.

We will also look at the case where we have two interacting electrons in a three dimensional harmonic oscillator, with different frequencies.
\section{Method}
\subsection*{2a) Mathematical intermezzo}
We first want to look at the orthogonality properties of a specific basis. Let us consider the following basis of vectors
\begin{align}
\mathbf{v}_i = \begin{pmatrix}
v_{i1} \\
v_{i2} \\
\vdots \\
v_{in}
\end{pmatrix}
\end{align}
We will also assume that this is an orthogonal basis
\begin{align}
\mathbf{v}_j^T\mathbf{v}_i = \delta_{ij}
\end{align}
We want to look at the orthogonality properties for a orthogonal and unitary vector transformation in the form
\begin{align}
\mathbf{w}_i = \mathbf{U}\mathbf{v}_i
\end{align}
For an orthogonal matrix, we know that $\mathbf{U}^T\mathbf{U} = \mathbf{I}=1$, where $\mathbf{I}$ is the identity matrix, so we have that
\begin{align}
\mathbf{w}_j^T\mathbf{w}_i = (\mathbf{U}\mathbf{v}_i)^T(\mathbf{U}\mathbf{v}_i) = \mathbf{v}_j^T \mathbf{U}^T\mathbf{U}\mathbf{v}_i = \mathbf{v}_i^T\mathbf{v}_i = \delta{ij}
\end{align}
For a unitary matrix, we have that $\mathbf{U^*}\mathbf{U}=\mathbf{I}$ and $\mathbf{U}^{\dagger}\mathbf{U} = \mathbf{I}$, where $\mathbf{U}^{\dagger} = (\mathbf{U}^*)^T$ is the hermitian conjugate. Doing the hermitian conjugate on the transformation vector, we can show that
\begin{align}
\mathbf{w}_j^{\dagger}\mathbf{w}_i = (\mathbf{U}\mathbf{v}_j)^{\dagger}(\mathbf{U}\mathbf{v}_i) = \mathbf{v}_j^{\dagger}\mathbf{U}^{\dagger}\mathbf{U}\mathbf{v}_i = \mathbf{v}_j^{\dagger}\mathbf{v}_i = \delta_{ij}
\end{align}
Here we have assumed that the basis vector is real, so $\mathbf{v}_i^* = \mathbf{v}_i$.
\subsection*{2b) Jacobi's rotation method}
Jacobi's rotation method (or Jacobi's method) is an algorithm that can be used to solve the eigenvalue problem. Consider an $n\times n$ orthogonal transformation matrix
\begin{align}
\mathbf{S} = \begin{pmatrix}
1 & 0 & \cdots & 0 & 0 & \cdots & 0 & 0 \\
0 & 1 & \cdots & 0 & 0 & \cdots & 0 & 0 \\
\cdots & \cdots & \cdots & \cdots &\cdots & \cdots & \cdots & \cdots \\
0 & 0& \cdots & \cos \theta & 0 & \cdots & 0 & \sin \theta \\
0 & 0 & \cdots & 0 & 1 & \cdots & 0 & 0 \\
0 & \cdots & \cdots & \cdots & \cdots \\
0 & 0 & \cdots & 0 & 0 & \cdots & 1 & 0 \\
0 & 0 & \cdots & -\sin\theta & 0 & \cdots & 0 & \cos\theta
\end{pmatrix}
\end{align}
with the property $\mathbf{S}^T = \mathbf{S}^{-1}$. A similarity transformation, on a matrix $\mathbf{A}$
\begin{align}
\mathbf{B} = \mathbf{S}^T \mathbf{A} \mathbf{S}
\end{align}
gives us 
\begin{align}
b_{ii} &= a_{ii} ,\quad i\neq k, i\neq l \\
b_{ik} &= a_{ik} \cos\theta - a_{il}\sin\theta, \quad i\neq k, i\neq j \\
b_{il} &= a_{il}\cos\theta + a_{ik}\sin\theta, \quad i\neq k, i\neq j \\
b_{kk} &= a_{kk}\cos^2\theta - 2a_{kl}\cos\theta\sin\theta + a_{ll}\sin^2\theta \\
b_{ll} &= a_{ll}\cos^2\theta + 2a_{kl}\cos\theta\sin\theta + a_{kk}\sin^2\theta \\
b_{kl} &= (a_{kk}-a_{ll})\cos\theta\sin\theta + a_{kl}(\cos^2\theta - \sin^2\theta)
\end{align}
The angle $\theta$ is arbitrary. The idea here is to choose a $\theta$ that causes all non-diagonal matrix elements to become zero (or within a given tolerance). However, not all non-diagonal matrix elements will become zero after one iteration, so we will have to run this algorithm a sufficient amount of times to get the desired result.

First we have to choose a value $a_{ij}$. This value will be the largest non-diagonal value in the matrix $\mathbf{A}$ and needs to be larger within a tolerance $\epsilon$, that is
\begin{align}
\text{max}(a_{ij}^2) > \epsilon
\end{align}
Since we want the non-diagonal matrix elements to become zero, we require that $b_{kl} = b_{lk} = 0$, which gives
\begin{align}
b_{kl} = (a_{kk} - a_{ll})\cos\theta\sin\theta + a_{kl}(\cos^2\theta - \sin^2\theta) = 0
\end{align}
Defining $\tau = \frac{a_{ll}-a_{kk}}{2a_{kl}}$ and $\tan\theta = t = \frac{\sin\theta}{\cos\theta}$, we can write the above expression as
\begin{align}
-2a_{kl}\tau t + \frac{a_{kl}}{\cos^2\theta}(\cos^2\theta - \sin^2\theta) &= 0 \nonumber \\
\implies -2\tau t + 1 - t^2 &= 0 \nonumber \\
\implies t^2 + 2\tau t - 1 &= 0
\end{align}
Where we have in the first line inserted the expression for $\tau$ and then divided by $\cos^2\theta$. We can now use this second order equation to determine $\tan\theta$.
\begin{align}
\tan\theta = -\tau \pm \sqrt{1+\tau^2}
\end{align} 
Once that is determined, one can easily calculate the new values of $\cos\theta$ and $\sin\theta$ given as
\begin{align}
\cos\theta &= \frac{1}{\sqrt{1 + \tan^2\theta}}\\
\sin\theta &= \cos\theta \tan\theta 
\end{align}
When these are calculated, we can compute the new values of $b$ which results in a new matrix. We then repeat the same procedure, with this new matrix, until
\begin{align}
\text{max}(a_{ij}^2) > \epsilon
\end{align}
is no longer satisfied. The end result is that the matrix $\mathbf{A}$ will only contain values along the diagonal, which will be the eigenvalues (or energies) of the system. Note that the eigenvalues along the diagonal are not ordered from the lowest to highest eigenvalue (or vice versa).

The reason we decided to choose the largest value of the non-diagonal elements is because of this test. It ensures that all the non-diagonal elements will be close to zero, thus giving us the diagonal matrix with the desired eigenvalues.

\subsection*{2c) Unit tests}
From exercise 2a, we have shown that the orthogonality is preserved for an orthogonal and/or unitary vector transformation. The Jacobi's method uses an orthogonal transformation, so one can expect that the resulting eigenvectors are remains orthogonal.

For a $n\times n$ matrix, we will have $n$ eigenvectors when we have diagonalized the matrix $\mathbf{A}$. We first have to calculate how many possible vector dot product combinations there are for all these $n$ eigenvectors. Let us first consider $n=2$, which gives us two eigenvectors so the only vector product combination we have is $\mathbf{v}_1 \cdot \mathbf{v}_2$. For $n = 3$ we have
\begin{align*}
\mathbf{v}_1 \cdot \mathbf{v}_2, \quad \mathbf{v}_1\cdot \mathbf{v}_3, \quad \mathbf{v}_2 \cdot \mathbf{v}_3
\end{align*}
That is three combinations, for $n=4$
\begin{align*}
\mathbf{v}_1 \cdot \mathbf{v}_2, \quad \mathbf{v}_1\cdot \mathbf{v}_3, \quad \mathbf{v}_1 \cdot \mathbf{v}_4\\
\mathbf{v}_2 \cdot \mathbf{v}_3, \quad \mathbf{v}_2 \cdot \mathbf{v}_3, \quad \mathbf{v}_3 \cdot \mathbf{v}_4
\end{align*}
Which is 6 combinations. For $n=5$ one can show that there are 10 combinations. We can clearly see a recursive pattern here, and can by this say that
\begin{align}
\text{Possible vector dot product combinations} = \frac{1}{2}n(n-1)
\end{align}
This will already give us an exact estimate on how many orthogonal combinations we can expect from the eigenvectors.
\section{Implementation}
For the C++ program, the first part was to implement the algorithm. I first create a function that initializes the matrix $\mathbf{A}$ as well as creating an identity matrix $\mathbf{R}$ which will contain the eigenvectors when $\mathbf{A}$ is fully diagonalized. In the Jacobi's method function, I also calculate the eigenvector after one iteration.

Setting the tolerance to $\epsilon = 10^{-8}$, I run the algorithm until $\text{max}(a_{ij}^2) > \epsilon$ is no longer satisfied. Each iteration is counted to give us an idea on how many iterations is required to diagonalize $\mathbf{A}$, which now contains the eigenvalues. As mentioned, the eigenvalues in the diagonalized matrix $\mathbf{A}$ is not ordered in ascending order, so I have to do it by using the \textit{sort} function from the \textit{algorithm} library.

For the interacting case, we use the exact same code but with different values of $\omega$. In this part I will also save the eigenvector for the ground state which is saved within the matrix $\mathbf{R}$. However, like the diagonalized matrix $\mathbf{A}$, the eigenvectors in $\mathbf{R}$ are also not ordered properly, so the program will have to find the eigenvector that corresponds to the ground state eigenvalue by doing certain tests. 

\section{Results}
In the C++ program, by selecting the number of mesh points to $n=400$, with $\rho_{max} = 10$, the output for the non interacting case is the following:
\begin{lstlisting}
@
Using a 400x400 matrix (n = 400)
with rho_max = 10

Max (absolute) value in the test matrix: 1000
max_diag does give out the largest value
max_diag result: 1000

Starting Jacobi's algorithm for a single electron
Time elapsed for non interactive case: 305.675s
Number of iterations: 260506
Lowest 3 eigenvalues are: 
2.99981
6.99903
10.9976
Possible number of orthogonal vectors: 79800
Number of orthogonal vectors: 79800
Number of non-orthogonal vectors: 0
\end{lstlisting}
The eigenvalues, even though they are not within 4 decimal precision, are very close to the exact analytical eigenvalues. We can also see that the number of iterations needed to fully diagonalize the matrix $\mathbf{A}$ (or at least within a tolerance $\epsilon$) was 260506 iterations. The unit test of the orthogonality between the eigenvectors is also shown, where every possibly combination of the scalar product between the eigenvectors are orthogonal (also within a tolerance). The unit test, where the largest matrix element in the test matrix was $-1000$, also worked out.

The time it took for the program to run was roughly 5 minutes. This seems like a very long simulation time, so there is probably something that is not quite optimal in the program, thus causing the program to run for a longer time.

Figure 1 shows the the probability distribution of the wave function (or the eigenvectors we calculated), which have been normalized, for different frequencies $\omega$.
\begin{figure}[hbtp]
\centering
\includegraphics[width=\linewidth]{Plots/Plot_groundstate_n400.pdf}
\caption{Probability distribution of the (normalized) wave functions of the ground state, for different values of $\omega$. Plotted for $n=400$ meshpoints.}
\end{figure}
These results are not quite surprising. Let us for a moment imagine that two electrons are bound together by a spring. The electrons oscillate back and forth due to the oscillation of the spring. They interact once when they are closest to each other (that is, the distance between them is the minimum).

For lower frequencies, the electrons will interact less frequently which means, by the analogy above, they are further apart from each other a lot more often. Another way to think of this is that the spring can be drawn over a larger distance before it is forced to contract, thus allowing the electrons to go further out. This can be seen from figure 1 (the blue line with $\omega = 0.01$) where the probability of them being close to each other is very small, while them being further apart is slightly larger.

For higher frequencies, the electrons will interact more frequently. Using the same analogy, we can think of the electrons bouncing back and forth very quickly. The spring is very stiff, so it cannot be stretched that far. Because of this, the electrons are forced to be very close to each other, which is exactly what we see in figure 1. In the case of $\omega = 5$ (the cyan line), we can clearly see that the probability of them being very close to each other is very large, while them being further apart is very low (practically zero).
\section{Conclusion}
The run time of the Jacobi's method is quite slow, which implies that this algorithm is not exactly the smartest algorithm to use in order to solve eigenvalue problems. This is especially true if we compare Jacobi's method to other algorithms.

Our eigenvalue results did not reach 4 decimal precision, but did come very close. One could possibly fix that by increasing the number of mesh points $n$ as well as $\rho_{max}$. However, as shown from the results, increasing $n$ would also increase the run time of the program, which is probably not practical for our purpose.
\section{References}
M. Hjort-Jensen, 2015, \textit{Computational Physics}, accessible at course GitHub repository; \url{https://github.com/CompPhysics/ComputationalPhysics/tree/master/doc/Lectures} (as of 14.09.16), 551 pages.
\end{document}