\documentclass[12pt]{article}
\author{Alex Ho}
\title{FYS4150 - Computational Physics \\ Project 4}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{float}
\usepackage[utf8]{inputenc}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{placeins}
\usepackage{parskip}
\setlength\parskip{\baselineskip}
\setlength\parindent{0pt}

\lstset{
language=Python,
basicstyle=\ttfamily,
otherkeywords={self},             
keywordstyle=\ttfamily\color{blue!90!black},
keywords=[2]{True,False},
keywordstyle={[2]\ttfamily\color{blue!90!black}},
emph={MyClass,__init__},          
emphstyle=\ttfamily\color{red!80!black},    
stringstyle=\color{blue!90!black},
showstringspaces=false,
commentstyle=\color{blue!90!black},
breaklines=true,
tabsize=3,
moredelim=**[is][\color{blue}]{@}{@}
}
\begin{document}
\maketitle
\begin{abstract}
We will take a look at the Ising model and implement the Metropolis method to solve the Ising model numerically. With this, we observe the behaviour of the energy and magnetization for different lattice sizes. We also will also study the behaviour of the Ising model near the critical temperature.
\end{abstract}
\newpage
\tableofcontents
\newpage
\section{Introduction} \label{section:intro}
In fields like thermal dynamics, one will study the phenomena called a \emph{phase transition}. Not only do we study it, but we experience this phenomena on a day-by-day basis. An example of a phase transition is when water turns to steam when we boil the water or when water freezes to ice, when it is very cold outside. More general, a phase transition is when matter changes it's form, either from gas to liquid, or liquid to solid (and vice versa).

We will in this project use a very popular model, the Ising model, to look at the statistical properties of phase transitions analytically. We will first consider a small $2\times 2$ lattice and see how the Metropolis algorithm compares to the analytical solutions for this system. After that, we will see how this system evolves with time as we increase the lattice size. Finally, we will take a look at the phase transition of a system with a finite magnetic moment to a system with (almost) zero magnetic moment, which will be done for different lattice sizes.

All relevant files can be found in this GitHub page:

\url{https://github.com/AHo94/FYS3150_Projects/tree/master/Project4}

\section{Method} \label{section:methods}
\subsection{The Ising model}
The Ising model, named after Ernst Ising, is a mathematical model for magnetic systems in statistical mechanics. It was originally motivated by the studies of ferromagnets. We will in this project consider phase transitions at finite temperatures. The energy, for a specific microstate $i$, is expressed as
\begin{align*}
E_i = -J \displaystyle \sum^N_{< kl >}s_ks_l - \mathcal{B}\sum^N_k s_k
\end{align*}
With the spins $s_k$ which can take values $s_k = \pm 1$. $N$ is the total number of spins in the system and $J$ is a coupling constant, which we will assume to be $J>0$. The symbol $< kl >$ indicates that we only sum over the nearest neighbouring spin. That is, we only the spin $s_k$ with the closest spin $s_l$. $\mathcal{B}$ is an external magnetic field that is interacting with the magnetic moment between neighbouring spins. For simplicity of this project, we will set $\mathcal{B} = 0$.\\\\
We also have the magnetization (or magnetic moment) defined as
\begin{align*}
\mathcal{M} = \displaystyle \sum^N_{j=1} s_j
\end{align*}
What we are interested in is the expectation values of the energy $\langle E \rangle$ and the magnetization $\langle \mathcal{M} \rangle$. In order to do that, we will need a probability distribution
\begin{align*}
P_i(\beta) = \frac{e^{-\beta E_i}}{Z}
\end{align*}
where $\beta = 1/k_bT$, the inverse temperature with $k_b$ as the Boltzmann constant. The partition function $Z$ is given as
\begin{align}
Z = \displaystyle \sum_{i=1}^M e^{-\beta E_i}
\label{eq:Partition_func}
\end{align}
which sums over all possible microstates $M$. With these, we can calculate the expectation value of an arbitrary variable $x$
\begin{align*}
\langle x \rangle = \frac{1}{Z}\sum_i x_i e^{-\beta E_i} \\
\langle x^2 \rangle = \frac{1}{Z}\sum_i x_i^2 e^{-\beta E_i}
\end{align*} 
The variance for this variable is then
\begin{align*}
\sigma_x^2 = \langle x^2 \rangle - \langle x \rangle^2
\end{align*}
We can now use these properties to calculate the heat capacity $C_v$ and susceptibility $\chi$, which are given as
\begin{align*}
C_v &= \frac{\sigma_E^2}{k_BT^2} \\
\chi &= \frac{\sigma_M^2}{k_BT}
\end{align*}


\subsection{Metropolis algorithm and computation time}
Monte Carlo methods are widely used to solve problems in different fields in science. The idea behind Monte Carlo simulation is do statistical simulations by drawing a sequence of random numbers In this project, we will use the Monte Carlo method known as the Metropolis algorithm, which is one of the top 10 algorithms, to solve the Ising model numerically.
The algorithm are as follows: for each Monte Carlo cycle we do:
\begin{itemize}
\item \textbf{1)} We start with an arbitrary microstate for the given lattice. We use that microstate to calculate the energy $E_b$
\item \textbf{2)} Choose a random spin in the microstate and flip it (e.g, from $\uparrow$ to $\downarrow$). Now we use that state to calculate the energy $E_t$.
\item \textbf{3)} Calculate the energy difference $\Delta E = E_t - E_b$. 
\item \textbf{4)} If $\Delta E <= 0$, we accept the new spin configuration and use the energy $E_t$. What this means is that the energy has been lowered and we are in a lower state.
\item \textbf{5)} If $ \Delta E > 0$, pick a random number $r$ that is given from a uniform distributor in the range $r \in [0,1]$. Now we have to consider these two cases
\item \textbf{5a)} If $r <= e^{-\beta \Delta E}$, we accept the new spin configuration and use the energy $E_t$.
\item \textbf{5b)} If $ r > e^{-\beta \Delta E}$, we reject this new configuration and flip the chosen spin back to it's original state and use the energy $E_b$.
\item \textbf{6)} Once that is done, we use the given microstate (depending on the cases given above) to calculate the magnetic moment $M_i$.
\item \textbf{7)} Finally, we add the calculated energy and magnetic moment to $E_{sum}$ and $M_{sum}$ respectively. These will the one sample of the Monte Carlo cycle.
\end{itemize}
We repeat this process $N_{mc}$ times (with $N_{mc}$ as the number of Monte Carlo cycles) and multiple samplings which will be summed into $E_{sum}$ and $M_{sum}$. Once that is done, we get the mean energies as
\begin{align*}
\langle E \rangle = \frac{1}{N_{mc}}E_{sum} = \frac{1}{N_{mc}}\displaystyle \sum_i E_i
\end{align*}
With $E_i$ is energy sample of the $i$'th Monte Carlo cycle. Similar expressions are used for the magnetic moment and their squares.

Since we are working with probabilities, we will have to run this algorithm many, many times. Take a dice roll as an example. We know that the probability of getting the number 3 is $1/6$. If we were to throw the dice 10 times, the probability of getting 3 will most likely not be $1/6$ any more. One have to throw the die many many times to get the expected probability. The same idea is applied for Monte Carlo methods, as one has to do many samplings to get a reasonable result. This is also why Monte Carlo simulations are known to be greedy simulations (in terms of computation time).

Because of this, doing Monte Carlo simulation will take a lot of computation time. Since we have to change all the possible spins $L$, the number of floating point operations our algorithm will be roughly $L^2$. This is not including other intermediate calculations within the algorithm. When we consider $L = 2$, the computation time will be incredibly short. However, as we increase the total number of spins to $L = 20, 40, 60, 100, 140$, the computation time will increase by roughly $400, 1600, 3600, 10 000$ and $19 600$ times respectively.

Note that this is only for one Monte Carlo cycle. As mentioned earlier, we will have to do many Monte Carlo cycles to obtain a reasonable result. This will increase the computation time to roughly $N_{mc} L^2$. On top of that, we  will have to do these simulations for different temperature $T$, so the simulations will increase yet again to roughly $N_T N_{mc} L^2$, where $N_T$ is the number of temperatures we would like to have. To save time, we will have to do Parallel computation, which will explained in the next part.

\subsection{Parallel computation}
As mentioned in the previous part, there will be very long computation time once we increase the lattice size. In order to save time, we will have to use multiple cores, that should be provided in a modern computer or laptop, to do our simulation. The idea is to let the core work in parallel with each other, that is, to split the computation work among different cores, which will speed up our process quite significantly.

We will use openMPI to do parallel computing. One has to first specify the number of cores the computer has available, and openMPI will automatically split these cores to different nodes (or ranks). Each node will run the program separately, but we can specify certain initial numbers, or intervals, for each node. One can, for example, tell each node to do the computation for different temperatures. One can also specify which interval each nodes computes. For example, node 0 can compute the Monte Carlo cycles in the range $N_{mc} \in [0,100]$, while node 1 computes in the interval $N_{cm} \in [101, 200]$. If we were to do the latter option, we will have to keep in mind that the nodes are working separately, and the computed values will therefore be unknown to each other. We will have to merge the values together to one main node (or the master node), which can be done by using the command \texttt{MPI\_Reduce}.

The computation time will decrease proportionally with the number of cores we use for the simulation. The computation time will be roughly $(N_TN_{mc}L^2)/N_{c}$, where $N_{c}$ is the number of cores used in the computation. Most computers and laptops, that one can buy from a computer shop, usually contains 4 cores. Using all four of them will reduce the computation time by roughly 4.
\subsection{Simple 2 $\times$ 2 lattice}
We will first consider a 2$\times$2 lattice, find the analytical expression for partition function $Z$ and find the corresponding expectation value of energy $E$, mean magnetization $|\mathcal{M}|$, specific heat $C_V$ and susceptibility $\xi$ as functions of temperature $T$. We also consider periodic boundary condition for this lattice. Once we have the analytical expressions, we will compare them to the numerical values from the Metropolis algorithm.

For this system, we will assume that every spin has two directions, i.e. our states can be either be in spin up state or spin down state (shorthand notation as $\uparrow$ or $\downarrow$ respectively).

The energy of the Ising model, without an external magnetic field $\mathcal{B}$, is given by
\begin{align*}
E_i = \displaystyle -J \sum_{<kl>}^Ns_k s_l
\end{align*} 
Where $J > 0$ is a coupling constant and $N$ is the total number of spins. The symbol $<kl>$ indicates that we only sum over the neighbours only. The values $s_k = \pm 1$ depends on which state it is in. We let $s_{\downarrow} = -1$ and $s_{\uparrow} = 1$. We also have the magnetic moment is given as
\begin{align*}
\mathcal{M}_i = \displaystyle \sum_{<k>}^N s_k
\end{align*}
Since we have a $2\times2=4$ lattice, and we have two spin directions, then the number of microstate (or configuration) is $2^4 = 16$. What this means is that our we can have 16 different energies, as well as 16 different magnetic moment, for each respective microstate. Table \ref{table:All_microstates} shows all the possible microstates.

\begin{table}
\begin{center}
	\begin{tabular}{c c c c}
	Combinations of & ($s_1, s_2, s_3, s_4$)& $s_j = \lbrace \uparrow, \downarrow \rbrace$  = $\lbrace 1, -1 \rbrace$ &\\
	\hline 
	($\uparrow , \uparrow, \uparrow, \uparrow$) & 
	($\uparrow , \uparrow, \uparrow, \downarrow$) & 
	($\uparrow , \uparrow, \downarrow, \uparrow$)  & 
	($\uparrow , \downarrow, \uparrow, \uparrow$) \\
	($\downarrow , \uparrow, \uparrow, \uparrow$)& ($\uparrow, \uparrow, \downarrow, \downarrow$) & ($\uparrow, \downarrow, \uparrow, \downarrow$) & ($\downarrow, \uparrow, \uparrow, \downarrow$) \\
	($\downarrow, \uparrow, \downarrow, \uparrow$)& ($\downarrow, \downarrow, \uparrow, \uparrow$) & ($\uparrow, \downarrow, \downarrow, \uparrow$) & ($\uparrow, \downarrow, \downarrow, \downarrow$) \\
	($\downarrow, \uparrow, \downarrow, \downarrow$) & ($\downarrow, \downarrow, \uparrow, \downarrow$) & ($\downarrow, \downarrow, \downarrow, \uparrow$) & ($\downarrow, \downarrow, \downarrow, \downarrow$) \\
	\hline
	\end{tabular}
\caption{All the microstates possible.}
\label{table:All_microstates}
\end{center}
\end{table}

Figure \ref{fig:Lattice_illustration} shows a $2\times2$ lattice. We see that the point $s_1$ has $s_2$ and $s_3$ as the closest neighbours. Since we are considering periodic boundary conditions, then $s_1$ will connect to $s_2$ and $s_3$ twice. The energy term will then give the term $2(s_1s_2 + s_2s_3)$ for the point $s_1$. It does not include $s_4$ as it is not the closest neighbour to $s_1$. 
\begin{figure}[!h]
\centering
\includegraphics[width=\linewidth]{2x2_lattice_illustration.png}
\caption{An illustration of the $2\times 2$ lattice. The black points corresponds to the ordinary points $s_1, s_2, s_3, s_4$ (as point 1, 2, 3, 4 in the figure respectively). The blue points corresponds periodic boundary points.}
\label{fig:Lattice_illustration}
\end{figure}

We can then continue to add more terms using the three other points, but we need to be careful to not include the connections of the points we previously have considered, which is to prevent double counting. Doing this, the energy for each microstate $i$ will be
\begin{align}
E_i = -2J\displaystyle \sum_{s_1 = \pm1} \sum_{s_2 = \pm1} \sum_{s_3 = \pm1} \sum_{s_4 = \pm1}(s_1s_2 + s_1s_3 + s_2s_4 + s_3s_4)
\label{eq:Energy}
\end{align}
Similarly for the magnetic moment we get when we sum over all microstates 
\begin{align}
\mathcal{M}_i = \displaystyle \sum_{s_1 = \pm1} \sum_{s_2 = \pm1} \sum_{s_3 = \pm1} \sum_{s_4 = \pm1} (s_1 + s_2 + s_3 + s_4)
\label{eq:Magnetic_moment}
\end{align}
Let us now determine both the energies and magnetic moments for all microstates. Using table \ref{table:All_microstates}, we can determine equation (\ref{eq:Energy}) and (\ref{eq:Magnetic_moment}) to their respective microstate. Table \ref{table:All_energies} shows the energies and magnetic moments based on how many spins are up spins. The degeneracy indicates how many times the energy and magnetic moment occurs for the given number of up spins.

\begin{table}
\begin{center}
	\begin{tabular}{|l| l| r| r |}
	Number of spins up& Degeneracy & Energy & Magnetic moment\\
	\hline
	4 & 1 & $-8J$ & 4\\
	3 & 4 & 0 & 2\\
	2 & 4 & 0 & 0\\
	2 & 2 & $8J$ & 0\\
	1 & 4 & 0 & -2\\
	0 & 1 & $-8J$ & 4 \\
	\hline
	\end{tabular}
\caption{Energies and magnetic moment based on how many spins are pointing up. Degeneracy means how many times the energy or magnetization it occurs.}
\label{table:All_energies}
\end{center}
\end{table}

Now that we have the energies of each microstate, we can find an analytical expression for the partition function $Z$ given in \ref{eq:Partition_func}. Using the energies given in table \ref{table:All_energies}, the partition function becomes
\begin{align*}
Z = 2e^{8\beta J} + 2e^{-8\beta J} + 12 e^0 = 4\cosh(8\beta J) + 12
\end{align*}
With the partition function, we can calculate the expectation value of the energy
\begin{align*}
\langle E\rangle = \frac{1}{Z}\displaystyle \sum_i E_ie^{-\beta E_i}
\end{align*}
Summing over all states $i$, with the given energies in table \ref{table:All_energies}, we get
\begin{align*}
\langle E \rangle &= \frac{1}{Z}\left( 2(8J)e^{-8\beta J} + 2(-8J)e^{8\beta J} + 12\times0\times e^0 \right)\\
&= \frac{-32J \sinh(8\beta J)}{4\cosh(8\beta J) + 12}\\
&= \frac{-8J\sinh(8\beta J)}{\cosh(8\beta J) + 3}
\end{align*}
The expectation of the energy squared is then
\begin{align*}
\langle E^2\rangle &= \frac{1}{Z}\displaystyle \sum_i E_i^2 e^{-\beta E_i} \\
&= \frac{1}{Z} \left(2(8J)^2 e^{-8\beta J} + 2(-8J)^2 e^{8\beta J} + 12 \times (0)^2 e^0 \right) \\
&= \frac{4(8J)^2\cosh(8\beta J)}{4\cosh(8\beta J) + 12} \\
&= \frac{64J^2\cosh(8\beta J)}{\cosh(8\beta J) + 3}
\end{align*}
The standard deviation of the energy then becomes
\begin{align*}
\sigma_E^2 &= \langle E^2\rangle - \langle E \rangle^2 \\
&=\frac{64J^2\cosh(8\beta J)}{\cosh(8\beta J) + 3} - \left( \frac{-8J\sinh(8\beta J)}{\cosh(8\beta J) + 3}\right)^2 \\
&= \frac{64J^2}{\cosh(8\beta J) + 3}\left(\cosh(8\beta J) - \frac{\sinh^2(8\beta J)}{\cosh(8\beta J) + 3} \right) \\
&= \frac{64J^2}{\cosh(8\beta J) + 3}\left(\frac{\cosh^2(8\beta J) + 3\cosh(8\beta J)}{\cosh(8\beta J) + 3} - \frac{\sinh^2(8\beta J)}{\cosh(8\beta J) + 3} \right) \\
&= \frac{64J^2}{\cosh(8\beta J) + 3}\left(\frac{1 + 3\cosh(8\beta J)}{\cosh(8\beta J) + 3} \right)\\
&= \frac{64J^2}{(\cosh(8\beta J) + 3)^2}(4+\cosh(8\beta J))
\end{align*}
Dividing by $k_B T^2$ gives us the specific heat $C_V$
\begin{align}
C_V = \frac{1}{k_BT^2} \left( \langle E^2 \rangle
- \langle E \rangle^2 \right) = \frac{64J^2}{k_B T^2} \frac{(1+3\cosh(8\beta J))}{(\cosh(8\beta J) + 3)^2}
\label{eq:heat_capacity}
\end{align}
We can do similar calculations to obtain the mean magnetization (or the mean absolute value of the magnetic moment), which then gives us
\begin{align*}
\langle |\mathcal{M}|\rangle &= \frac{1}{Z} \displaystyle \sum_i |\mathcal{M}_i| e^{-\beta E_i} = \frac{2(e^{8\beta J} + 2)}{\cosh(8\beta J) + 3} \\
\langle \mathcal{M}^2\rangle &= \frac{1}{Z}\displaystyle \sum_i \mathcal{M}_i^2e^{-\beta E_i} = \frac{8(e^{8\beta J} + 1)}{\cosh(8\beta J) + 3}
\end{align*}
Which we can use to calculate the susceptibility $\chi$
\begin{align}
\chi = \frac{1}{k_B T} \left(\langle \mathcal{M}^2 \rangle - \langle |\mathcal{M}| \rangle^2\right) = \frac{8}{k_B T} \frac{(e^{8\beta J}+ \cosh(8\beta J) + \frac{3}{2}))}{(\cosh(8\beta J) + 3)^2}
\label{eq:suceptibility}
\end{align} 
Now that we have the analytical expressions, we will later compare it to the numerical values (for a given temperature) from the Metropolis algorithm.
\FloatBarrier

\subsection{Studies of phase transition}
Once we have properly tested the Ising model by using the Metropolis algorithm, we will study phase transitions. A phase transition is the transition between solid, liquid or gas states of the matter. An example of this is water boiling to steam, or water freezing to ice. We can study this by studying the behaviour of the Ising model near the critical temperature. We are also interested in the critical temperature for infinitely large lattices.

The system will undergo a phase transition when it reaches the critical temperature $T_C$. Our system will initially start with a finite magnetization. Once the system reaches the critical temperature, and undergoes the phase transition, we will expect that the system will have almost zero magnetization.

Through so-called finite size scaling relations, we can relate the behaviour of finite sized lattices with the results of an infinitely large lattice. The critical temperature then scales as
\begin{align}
T_C(L) - T_C(L=\infty) = a L^{-1/\nu}
\label{eq:crit_temp_scaling}
\end{align}
Where $a$ is a constant, and $\nu$ is defined from
\begin{align*}
\xi(T) ~ |T_C - T|^{-\nu}
\end{align*}
For our case, we will set $\nu = 1$. This is to compare the exact result of the critical temperature after Lars Onsager, which is.
\begin{align}
\frac{kT_C}{J} = \frac{1}{\ln(1 + \sqrt{2})} \approx 2.269
\label{eq:Crit_temp_infty_exact}
\end{align}
With $\nu = 1$. First, we will have to find what this constant $a$ is. By taking the difference of equation \ref{eq:crit_temp_scaling} for two different total spins, we have
\begin{align*}
aL_i^{-1} - aL_j^{-1} &= [T_C(L_i) - T_C(L=\infty)] - [T_C(L_j) - T_C(L = \infty)] \\
&= T_C(L_i) - T_C(L_j)\\
\implies a(L_i, L_j) &= \frac{T_C(L_i) - T_C(L_j)}{L_i^{-1} - L_j^{-1}}
\end{align*}
The critical temperature for an infinitely large lattice, for a given total spin $L_i$, is then
\begin{align*}
T_C(L=\infty) &= T_C(L_i) - L_i^{-1}\frac{T_C(L_i) - T_C(L_j)}{L_i^{-1} - L_j^{-1}} \\
&= T_C(L_i) - \frac{T_C(L_i) - T_C(L_j)}{1 - \left(\frac{L_i}{L_j} \right)}
\end{align*}
We will have to sum over all possible combinations of $L_i$ and $L_j$. For this case, we have to sum the values of $a(40, 60), a(40, 100), a(40,140), a(60,100)$ etc. When we sum over all these combinations, we will need to normalize the number of times we have summed over these $a(L_i, L_j)$'s. The final expression is then
\begin{align}
T_C(L = \infty) = \frac{1}{N_a}\displaystyle \sum_i \sum_{j\neq i} \left( T_C(L_i) - \frac{T_C(L_i) - T_C(L_j)}{1 - \left( \frac{L_i}{L_j} \right)} \right)
\label{eq:Crit_temp_infty}
\end{align}
Where $N_a$ is the number of times we have summed over $a(L_i, L_j)$. 

\section{Implementation} \label{section:implement}
All programming are done in C++ and data will be saved in a text file. The plotting part will be done in Python.

The main function in the C++ program is split into two parts by an if test. The if test will see if there is any arguments given in the command line. If there is none, the program will compute the simple lattice model for $L = 2$ and $L = 20$. If the there are arguments in the command line, the program will run the program for large lattices. The program will here do parallel computation, where I have decided to separate the Monte Carlo cycle interval for each node. 

There are also various of functions that is used to do all the calculations. Some of the functions have relatively similar names. One example is the \texttt{write\_file} functions. I have made two (or three, if you include the function that initializes the output file) of these functions. Both creates differently formatted output files, which is done because it will be easier to read the required data for the different tasks. 

When we have to draw random numbers, when doing the Monte Carlo computations, my program have set a seed that depends on the time of the computer. This will ensure that the random numbers will always be different each time we run the program.

Like my previous projects, I will plot all the data, made from the C++ program, in Python. The Python script simply reads the output file, saves the required data and then plots them.
\section{Results} \label{section:result}
\subsection{Comparing with the analytical solution}
We will now compare the numerical values for $C_v$ and $\chi$ with their respective analytical values given in equation \ref{eq:heat_capacity} and \ref{eq:suceptibility}. For this case, we will use the temperature $T = 1.0$ in units of $kT/J$. The coupling constant $J$ will be set to $J=1$ throughout the whole project. Running this in C++, with $10^6$ Monte Carlo cycles we obtain the following output (Note that the numerical values has not been divided by the number of spins):

\begin{lstlisting}
@Number of Monte Carlo cycles = 1000000
Analytic C_v = 0.128329, Numerical C_v = 0.128445
Analytic Chi = 0.016043, Numerical Chi = 0.0162828@
\end{lstlisting}
As we can see, the values are almost identical. The numerical values may change a little bit when we run this multiple times, but still remains fairly close to the analytical values. An example of this can be found in the output below
\begin{lstlisting}
@Running multiple times, using MC_cycles = 1000000
Analytic C_v = 0.128329, Numerical C_v = 0.123027
Analytic Chi = 0.016043, Numerical Chi = 0.0156212
 
Analytic C_v = 0.128329, Numerical C_v = 0.127936
Analytic Chi = 0.016043, Numerical Chi = 0.0154724
 
Analytic C_v = 0.128329, Numerical C_v = 0.130357
Analytic Chi = 0.016043, Numerical Chi = 0.0164741
 
Analytic C_v = 0.128329, Numerical C_v = 0.125385
Analytic Chi = 0.016043, Numerical Chi = 0.0152174
 
Analytic C_v = 0.128329, Numerical C_v = 0.126915
Analytic Chi = 0.016043, Numerical Chi = 0.0156962
 
Analytic C_v = 0.128329, Numerical C_v = 0.124365
Analytic Chi = 0.016043, Numerical Chi = 0.0159204@
\end{lstlisting}
Using $10^5$ Monte Carlo cycles can also give a good numerical result, but it is not as consistent compared to $10^6$ Monte Carlo cycles. What that means is that the difference between the analytical and numerical values (for both $C_v$ and $\chi$) may be larger for a lower number of Monte Carlo cycles. The output below shows an example with $N_{mc} = 10^5$. Notice how, for example, the numerical values of $C_v$ can be as low as $0.112442$ and as high as $0.160237$. We can therefore safely say that we need roughly $10^6$ Monte Carlo cycles in order to achieve a good agreement with the analytical results.
\begin{lstlisting}
@Running multiple times, using MC_cycles = 100000
Analytic C_v = 0.128329, Numerical C_v = 0.125194
Analytic Chi = 0.016043, Numerical Chi = 0.0163315
 
Analytic C_v = 0.128329, Numerical C_v = 0.112442
Analytic Chi = 0.016043, Numerical Chi = 0.0130196
 
Analytic C_v = 0.128329, Numerical C_v = 0.123282
Analytic Chi = 0.016043, Numerical Chi = 0.0152538
 
Analytic C_v = 0.128329, Numerical C_v = 0.137304
Analytic Chi = 0.016043, Numerical Chi = 0.0180454
 
Analytic C_v = 0.128329, Numerical C_v = 0.125832
Analytic Chi = 0.016043, Numerical Chi = 0.0161317
 
Analytic C_v = 0.128329, Numerical C_v = 0.160237
Analytic Chi = 0.016043, Numerical Chi = 0.0194764@
\end{lstlisting}

\subsection{Lattice with L = 20}
We will now increase our lattice to a $20\times 20$ lattice.  For this task, we will have a look at how the mean energy and mean magnetization (absolute value) will reach an equilibrium as a function of the number of Monte Carlo cycles. Monte Carlo cycles will represent the time for this case. This will be done for temperatures at $T=1.0$ and $T = 2.4$. We will also consider two different cases. One where all the spins in the system is randomly set and one where all spins are pointing up. Note, all the plots for this part are not divided by the total number of spins.
 
Let us first consider the case where the initial spin state is set at random. Figure \ref{fig:Energy_stab_log_T1} and \ref{fig:Mag_stab_log_T1} are plots of the expectation values of energy and magnetization respectively, as a function of time (the number of Monte Carlo cycles) for $T = 1.0$. The x axis is plotted in logarithmic scales to better show how the  expectation values stabilizes over time. Non-logarithmic x-axis of these plots can be found in the appendix (also found in my GitHub page). We see that both parameters stabilizes fairly quickly, after roughly $10^2$ Monte Carlo cycles, even though we start in a random spin state.

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{Plots/Energy_stability_logarithmic_T1.pdf}
\caption{Stability of the energy, with $T = 1.0$ as a function of time. Logarithmic x-axis. Random initial state.}
\label{fig:Energy_stab_log_T1}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{Plots/Magnetization_stability_logarithmic_T1.pdf}
\caption{Stability of the magnetization, with $T = 1.0$ as a function of time. Logarithmic x-axis. Random initial state.}
\label{fig:Mag_stab_log_T1}
\end{figure}

We now increase the temperature to $T = 2.4$. Figure \ref{fig:Energy_stab_log_T2} and \ref{fig:Mag_stab_log_T2} again shows how the energy and magnetization stabilizes respectively. This time, however, there are a lot more fluctuations for both parameters. This is not surprising. When we have a higher temperature, the system can reach a higher energetic state. Because of this, the spins will have the ability to change more (that is, more frequent flips) compared to the lower temperature case. Also note that the energy oscillates around $\langle E \rangle = -500$ instead of $\langle E \rangle = -500$ (for the lower energy case). Again, due to an increase in temperature, the system will therefore be in an excited state, which means it has a larger energy on average. We also note that the magnetization has now become smaller due to the increased temperature.

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{Plots/Energy_stability_logarithmic_T24.pdf}
\caption{Stability of the energy, with $T = 2.4$ as a function of time. Logarithmic x-axis. Random initial state.}
\label{fig:Energy_stab_log_T2}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{Plots/Magnetization_stability_logarithmic_T24.pdf}
\caption{Stability of the magnetization, with $T = 2.4$ as a function of time. Logarithmic x-axis. Random initial state.}
\label{fig:Mag_stab_log_T2}
\end{figure}


We can also have the program count how many times the a new configuration have been accepted. In figure \ref{fig:Accepted_configs_wrt_temp}, we see that we accept a lot more configurations as the temperature increases. The y-axis for this plot is in logarithmic scale. Note that I have only plotted two temperature points in the plot, so the number of accepted configurations may or may not increase linearly with temperature. This result agrees with the stability plots for $T=2.4$ that we saw earlier. 

Figure \ref{fig:Accepted_configs_wrt_MC_cycles} shows how the number of accepted configurations increases over time. The y-axis here is also logarithmic. Once again, we see that the number of accepted configurations is larger for a higher temperature. Notice how we accept a lot more configuration changes at early times, and as time passes, we accept less. This indicates that the system is going towards equilibrium. 

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{Plots/Accepted_configs_wrt_temp.pdf}
\caption{Number of accepted configurations for the two different temperatures. Logarithmic y-axis. Shows only two temperature points, so the increase may not be linear.}
\label{fig:Accepted_configs_wrt_temp}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{Plots/Accepted_configurations_wrt_MC_cycles.pdf}
\caption{Number of accepted configuration changes over time. Logarithmic y-axis. We see that the number of accepted configurations increases as the temperature increases}
\label{fig:Accepted_configs_wrt_MC_cycles}
\end{figure}
If we now let all spins in the initial state be up spins, we would expect that the system will reach equilibrium a lot faster than a random initial state. The reason is that the energy will be at the lowest for a system with only up spins (also for down spins), which was shown previously in table \ref{table:All_energies}. Because of this, the energy (as well as the magnetization) will already start near their equilibrium value.

This can easily be seen in figure \ref{fig:Energy_stab_log_T1_SpinUp} and \ref{fig:Mag_stab_log_T1_SpinUp} for the energy and magnetization respectively. Let us look at the plot for energy as an example. The initial energy is very close to the lowest possible energy for our system. On the other hand, in figure \ref{fig:Energy_stab_log_T1}, the initial energy was larger due to an random initial state. The system will therefore reach an equilibrium state much faster when the system starts will all spins up.
\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{Plots/Energy_stability_UpInitSpin_logarithmic_T1.pdf}
\caption{Stability for the energy for an initial state where all spins points up, with $T=1.0$. Notice the lower initial energy, which is already close to the equilibrium energy.}
\label{fig:Energy_stab_log_T1_SpinUp}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{Plots/Magnetization_stability_UpInitSpin_logarithmic_T1.pdf}
\caption{Stability of the magnetization for an initiale state where all spins points up, with $T=1.0$. Like the energy case, the magnetization has an initial value close to equilibrium.}
\label{fig:Mag_stab_log_T1_SpinUp}
\end{figure}
We can also look at the case with $T=2.4$. Again, since the temperature is higher, the system can reach many more states  compared to the lower energy case. Recall that that the spin state will change it's configuration much more frequently for larger temperatures, so we can also expect more oscillations for the energy and magnetization in this system, where all spins starts as up spins.

In figure \ref{fig:Energy_stab_log_T2_SpinUp}, we can see that the energy oscillates around $\langle E \rangle = -500$. This is also not surprising due to a larger temperature, which means that the system will be in an excited state. Similar result can be seen for the magnetization in figure \ref{fig:Mag_stab_log_T2_SpinUp}.
\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{Plots/Energy_stability_UpInitSpin_logarithmic_T24.pdf}
\caption{Stability for the energy for an initial state where all spins points up, with $T=2.4$. }
\label{fig:Energy_stab_log_T2_SpinUp}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{Plots/Magnetization_stability_UpInitSpin_logarithmic_T24.pdf}
\caption{Stability of the magnetization for an initiale state where all spins points up, with $T=2.4$.}
\label{fig:Mag_stab_log_T2_SpinUp}
\end{figure}

\FloatBarrier
\subsection{Probability distribution}
Instead of plotting how the energy evolves over time (as we did in the previous part), we can instead plot the probability distribution of the energy for different temperatures. We know that there is a higher probability of the energy being larger when the temperature is larger. Let us try to count how many times an energy has occurred during the computation and then plot a histogram of the result.

A plot of the probability distribution for both $T=1.0$ and $T=2.4$ can be found in \ref{fig:prob_dist_merged}. As we can see, when the temperature is lower, the energy has a higher probability of being in a lower energetic state. When the temperature is larger, the energy can be at a more energetic state. Plots where the histograms are separated can be found in the appendix.

The variance, in the case of $T=2.4$, will be larger than the case of $T=1.0$. What this means is that the system for $T=2.4$ has many more energy states to choose from, and each of them has roughly the same probability. As we mentioned in the previous part, the system can reach many more states when it has a higher temperature, which corresponds to different energies. The result of this also agrees with the results from figure \ref{fig:Energy_stab_log_T2} and \ref{fig:Energy_stab_log_T2_SpinUp}. The energies for those cases were oscillating around $\langle E \rangle = -500$, and the histogram shows the exact same result.
\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{Plots/Probability_distribution_merged.pdf}
\caption{Probability distribution of the energies for $T=1.0$ (blue) and $T=2.4$ (green).}
\label{fig:prob_dist_merged}
\end{figure}
We are also tasked to compute the variance of the energies. I have computed the standard variance from the C++ program and I will compare it to the variance given from the Python (Numpy) function \texttt{numpy.var}. This is done by saving all the energies (that was counted in the C++ program) in an array and then just call the variance function from Numpy. The results are found in the following output:
\begin{lstlisting}
@Computed variance =  10.2423 , for T = 1.0
Computed variance =  3214.74 , for T = 2.4
Numpy variance =  31.11786096 ,for T = 1.0
Numpy variance =  3121.04932864 ,for T = 2.4@
\end{lstlisting}
The variance for $T=2.4$ from Numpy is very close to the computed value. However, they are not quite in agreement for $T=1.0$. The reasons are unknown to me, and I really doubt that there are any errors in the Numpy function, which means that there has probably been a computation error from my part. As we saw from the histogram, the system with a higher temperature had more energies to choose from. This results to a larger variance, which is exactly what got.
\FloatBarrier
\subsection{Phase transitions}
We will now study phase transitions within the Ising model. This can be done by running the Metropolis method for multiple temperature values. In this case, I have decided to choose the temperature range of $T\in [2.1, 2.35]$. The temperature step length will be $\Delta T = 0.05$. However, the interesting effects will happen near $T=2.2$, so I have told my program to switch the temperature step length to $\Delta T = 0.02$ in the interval $T\in[2.2,2.3]$. Grids sizes we consider are $L=40,60,100,140$. I will also use $10^6$ Monte Carlo cycles for every grid sizes.

Before I show the results, we will have to check if everything works correctly when we parallelize the program. We will waste many computation hours if the program turns out to be wrong, and running it for the $L=140$ case, using 4 CPUs, would take at least 12 hours. \footnote{In fact, I did the same mistake myself, where my values of $\chi$ were negative, after running it for the $L=140$ case.} I have tested this for the $L=20$ case, and the results can be found in this GitHub link:
 \url{https://github.com/AHo94/FYS3150_Projects/blob/master/Project4/build-Project4_cpp_program-Desktop_Qt_5_7_0_GCC_64bit-Debug/4e_data_L20.txt}

Comparing it with the results, provided by Morten H. Jensen in the following GitHub link: 
\url{https://github.com/CompPhysics/ComputationalPhysics/blob/master/doc/Programs/ParallelizationMPI/Results/Lattice20}, the values from my case are almost identical. However, the value of $\chi$ is different because Morten used the value of $\langle M \rangle$ instead of $\langle |M| \rangle$. We can, with relatively good confidence, assume that the results for larger lattices should also be correct.

Let us first see how the energy and magnetization evolves. We already know that the energy should be larger (and magnetization smaller) when the temperature increases. The plots of the energy and magnetization (per spin squared), for different grid sizes, can be found in figure \ref{fig:parallell_energy} and \ref{fig:parallell_magnetization} respectively. As we see, the energy increases and magnetization becomes very small as the temperature increases. These results agrees with our physical intuition and the results from our studies of the systems stability (section 4.2).
\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{Plots/Energy_parallellization.pdf}\caption{The energy per spin squared as a function of temperature for different grid sizes.}
\label{fig:parallell_energy}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{Plots/Magnetization_parallellization.pdf}
\caption{The magnetization per spin squared as a function of temperature for different grid sizes.}
\label{fig:parallell_magnetization}
\end{figure}

In order to study phase transitions, we will have to study how the heat capacity and susceptibility evolves when the temperature increases. Phase transitions happens when the temperature reaches the critical temperature. The critical temperature is when the heat capacity and susceptibility reaches a maximum. We can then plot our data and simply read it off the resulting graph. Plots of the heat capacity and susceptibility can be found in figure \ref{fig:parallell_heat_capacity} and \ref{fig:parallell_susceptiblity} respectively.

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{Plots/Heat_capacity_parallellization_interpolated.pdf}
\caption{Plot of the heat capacity per spin squared as a function of temperature. Includes different grid sizes. Also includes interpolated data.}
\label{fig:parallell_heat_capacity_interpolated}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{Plots/Susceptibility_parallellization_interpolated.pdf}
\caption{Plot of susceptibility per spin squared as function of temperature. Includes different grid sizes. Also includes interpolated data.}
\label{fig:parallell_susceptiblity_interpolated}
\end{figure}

Before we proceed, the maximum of both $C_V$ and $\chi$ will increase as the grid size increases. We should therefore use a lot more Monte Carlo cycles for the larger grids, in order to get a more precise result. However, running it for a larger amount of Monte Carlo would take too much time.

Another thing to keep in mind is that I have used very few temperature points for this task. More temperature points would, of course, give a better results. Again, it would take too much time to compute the data if we were to decrease the temperature step length. However, with our current data, we can interpolate the points near the maximum of $C_V$ and $\chi$. In fact, I have interpolated every point we have, to obtain a better result. This is done by using the Numpy function \texttt{numpy.interp}. Yet another thing to keep in mind is that interpolation may also not give correct answers, so the results may or may not be precise. Non-interpolated plots of $C_V$ and $\chi$ can be found in the appendix.

Let us finally take a look at how the heat capacity and susceptibility evolves as the temperature increases. Figure \ref{fig:parallell_susceptiblity_interpolated} and \ref{fig:parallell_susceptiblity_interpolated} shows plots of the heat capacity and susceptibility with their interpolated data. 

First thing we have to note is their odd values for low temperatures. For $L=140$, the susceptibility  starts near $\chi = 160$, which is a very large value. We would expect it to start at a lower value, just like the other grid sizes. The same thing can be said for the heat capacity, which should also have a lower value, in the case of $L=140$. Why this is the case is unknown to me, but it may be some errors within my program.

Even though the values we have weird values for low temperatures, the form of the graphs are reasonable. Both graphs has a peak near the critical temperature, which should indicate that something went right in the program near these temperatures. I will here just look at the graph to determine the maximum of both $C_V$ and $\chi$ and then determine the critical temperatures from that.

I will read off the critical temperature from the plot of the heat capacity. The rough values I get are as follows:
\begin{align*}
T_C(L=40) = 2.28874 \\
T_C(L=60) = 2.28119 \\
T_C(L=100) = 2.27788 \\
T_C(L=140) = 2.27788
\end{align*}
All in units of $k_bT/J$. I have calculated the critical temperature, as $L\to \infty$ in python. By plugging in the critical temperatures into a list, which is in ascending order corresponding to the total spins $L$, we can sum them over just like the analytical expression in equation \ref{eq:Crit_temp_infty}. We will have to also count how many possible combinations we have, which is counted by the variable \texttt{norm}. Once we have summed over all combinations and normalized, we get the following result:
\begin{lstlisting}
@Computed critical temperature for L -> infinity:
T_C(L = infty) =  2.52889166667@
\end{lstlisting}
This is very close to the exact solution given in equation \ref{eq:Crit_temp_infty_exact}. The small difference may be due to the fact that critical temperatures were read off manually from the plots. On top of that, we could have obtained a better result by decreasing the temperature step length as well as increasing the number of Monte Carlo cycles in the calculation.

\FloatBarrier
\section{Conclusion} \label{section:conclusion}
As we saw from the results, the Metropolis algorithm is a solid Monte Carlo algorithm. The computed value, when we compared it to a simple $2\times 2$ lattice, were almost identical to the analytical one. 

The algorithm also worked very well when we increased the lattice size. We saw that the computed energy were indeed larger for a larger temperature. This agrees with our physical intuition, where a system with a higher temperature is more energetic than a system with lower temperature. We can finally see why the Metropolis algorithm is considered one of the top 10 algorithms today.

The results that we obtained when we studied phase transitions were also fairly reasonable. Although, the results for the heat capacity and susceptibility were a little odd for the lower temperatures, as both their values were very large. The critical temperature, when $L\to \infty$ that we computed were also very close to the exact one, but the small error may be due to the fact that the critical  temperatures, for the different lattice sizes, were manually read off the plots.

Although, one can probably increase the number Monte Carlo cycles quite significantly for the larger lattices and even reduce the temperature step to obtain a more precise result, but that will increase the computation time quite considerably. One would have to gain access to a super computer, with many cores, if we wish to do the computation within a reasonable time frame.
\FloatBarrier

\section{Appendix}
All the figures given in the appendix can also be found in the GitHub page.
\url{https://github.com/AHo94/FYS3150_Projects/tree/master/Project4}
\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{Plots/Energy_stability_T1.pdf}
\caption{Stability of the energy for $T = 1.0$. Arbitrary initial spin state. Non-logarithmic x-axis.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{Plots/Magnetization_stability_T1.pdf}
\caption{Stability of the magnetization for $T=1.0$. Arbitrary initial spin state. Non-logarithmic x-axis.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{Plots/Energy_stability_T24.pdf}
\caption{Stability of the energy for $T=2.4$. Arbitrary initial spin state. Non-logarithmic x-axis.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{Plots/Magnetization_stability_T24.pdf}
\caption{Stability of the magnetization for $T=2.4$. Arbitrary initial spin state. Non-logarithmic x-axis.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{Plots/Energy_stability_UpInitSpin_T1.pdf}
\caption{Stability of the energy for $T = 1.0$. Initial state with all spins up. Non-logarithmic x-axis.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{Plots/Magnetization_stability_UpInitSpin_T1.pdf}
\caption{Stability of the magnetization for $T=1.0$. Initial state with all spins up. Non-logarithmic x-axis.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{Plots/Energy_stability_UpInitSpin_T24.pdf}
\caption{Stability of the energy for $T=2.4$. Initial state with all spins up. Non-logarithmic x-axis.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{Plots/Magnetization_stability_UpInitSpin_T24.pdf}
\caption{Stability of the magnetization for $T=2.4$. Initial state with all spins up. Non-logarithmic x-axis.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{Plots/Probability_distribution_T1.pdf}
\caption{Probability distribution for $T=1.0$.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{Plots/Probability_distribution_T2.pdf}
\caption{Probability distribution for $T=2.4$.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{Plots/Heat_capacity_parallellization.pdf}
\caption{Plot of the heat capacity per spin squared as a function of temperature. Includes different grid sizes. Non-interpolated plot.}
\label{fig:parallell_heat_capacity}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{Plots/Susceptibility_parallellization.pdf}
\caption{Plot of susceptibility per spin squared as function of temperature. Includes different grid sizes. Non-interpolated plot.}
\label{fig:parallell_susceptiblity}
\end{figure}

\FloatBarrier
\begin{thebibliography}{1}
    \bibitem{cpyhsics} M. Hjorth-Jensen, \emph{Computational Physics}, 2015, 551 pages
    \bibitem{thermphys} Daniel V. Schroeder, \emph{Thermal Physics}, 2000, 422 pages
\end{thebibliography}
\end{document}